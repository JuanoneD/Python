{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, activations, \\\n",
    "    optimizers, utils, losses, initializers, metrics, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "patience = 3\n",
    "learning_rate = 1e-4\n",
    "model_path = './model/sigmoid.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Resizing(128,128),\n",
    "    layers.Rescaling(1.0/255),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    layers.Conv2D(32,(5,5),\n",
    "        activation='relu',\n",
    "        kernel_initializer = initializers.RandomNormal()\n",
    "    ),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.BatchNormalization(),\n",
    "    \n",
    "    layers.Conv2D(64,(3,3),\n",
    "        activation='relu',\n",
    "        kernel_initializer = initializers.RandomNormal()\n",
    "    ),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "\n",
    "    layers.Conv2D(128,(3,3),\n",
    "        activation='relu',\n",
    "        kernel_initializer = initializers.RandomNormal()\n",
    "    ),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128,\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = initializers.RandomNormal()\n",
    "    ),\n",
    "    layers.Dense(64,\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = initializers.RandomNormal()\n",
    "    ),\n",
    "    layers.Dense(32,\n",
    "        activation = 'relu',\n",
    "        kernel_initializer = initializers.RandomNormal()\n",
    "    ),\n",
    "    layers.Dense(5,\n",
    "        activation = 'softmax',\n",
    "        kernel_initializer = initializers.RandomNormal()\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = optimizers.Adam(\n",
    "        learning_rate = learning_rate\n",
    "    ),\n",
    "    loss = losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3276 files belonging to 5 classes.\n",
      "Using 2621 files for training.\n",
      "Found 3276 files belonging to 5 classes.\n",
      "Using 655 files for validation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hammer', 'nothing', 'pliers', 'screwdriver', 'wrench']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = utils.image_dataset_from_directory(\n",
    "    'Data',\n",
    "    validation_split = 0.2,\n",
    "    subset = 'training',\n",
    "    seed = 123,\n",
    "    shuffle = True,\n",
    "    image_size = (512, 512),\n",
    "    batch_size = 32\n",
    ")\n",
    "test =  utils.image_dataset_from_directory(\n",
    "    'Data',\n",
    "    validation_split = 0.2,\n",
    "    subset = 'validation',\n",
    "    seed = 123,\n",
    "    shuffle = True,\n",
    "    image_size = (512, 512),\n",
    "    batch_size = 32\n",
    ")\n",
    "train.class_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 395ms/step - accuracy: 0.3273 - loss: 1.4646 - val_accuracy: 0.3557 - val_loss: 1.4762\n",
      "Epoch 2/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 401ms/step - accuracy: 0.4492 - loss: 1.1488 - val_accuracy: 0.3420 - val_loss: 1.3972\n",
      "Epoch 3/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 407ms/step - accuracy: 0.4955 - loss: 1.0909 - val_accuracy: 0.3588 - val_loss: 1.3197\n",
      "Epoch 4/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 372ms/step - accuracy: 0.5267 - loss: 1.0413 - val_accuracy: 0.4489 - val_loss: 1.1256\n",
      "Epoch 5/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 411ms/step - accuracy: 0.5645 - loss: 0.9853 - val_accuracy: 0.4779 - val_loss: 1.1175\n",
      "Epoch 6/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 384ms/step - accuracy: 0.5934 - loss: 0.9657 - val_accuracy: 0.5420 - val_loss: 1.0360\n",
      "Epoch 7/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 407ms/step - accuracy: 0.6155 - loss: 0.9074 - val_accuracy: 0.5710 - val_loss: 0.9884\n",
      "Epoch 8/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 462ms/step - accuracy: 0.6506 - loss: 0.8425 - val_accuracy: 0.5542 - val_loss: 0.9958\n",
      "Epoch 9/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 456ms/step - accuracy: 0.6632 - loss: 0.8116 - val_accuracy: 0.5603 - val_loss: 0.9899\n",
      "Epoch 10/100\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 403ms/step - accuracy: 0.6947 - loss: 0.7526 - val_accuracy: 0.5618 - val_loss: 1.0108\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d0b34c3260>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,\n",
    "    epochs = epochs,\n",
    "    validation_data = test,\n",
    "    callbacks= [\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss',\n",
    "            patience = patience,\n",
    "            verbose = 1\n",
    "        ),\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath = model_path,\n",
    "            save_weights_only = False,\n",
    "            monitor = 'loss',\n",
    "            mode = 'min',\n",
    "            save_best_only = True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    img = Image.open(img_path)\n",
    "    img_array = np.asarray(img)\n",
    "    return np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "[np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4)]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = []\n",
    "e.append(np.argmax(model.predict(load_img(\"data/hammer/000003.jpg\"))))\n",
    "e.append(np.argmax(model.predict(load_img(\"data/nothing/nothing1.jpg\"))))\n",
    "e.append(np.argmax(model.predict(load_img(\"data/Pliers/000000.jpg\"))))\n",
    "e.append(np.argmax(model.predict(load_img(\"data/screwdriver/000000.jpg\"))))\n",
    "e.append(np.argmax(model.predict(load_img(\"data/wrench/000000.jpg\"))))\n",
    "\n",
    "print(e)\n",
    "np.argmax(model.predict(load_img(\"3.jpeg\")))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
